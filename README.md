# CI.HLS

A configurable web scraping framework built with Scrapy for extracting news articles from multiple sources.


## Key Features

- ğŸ› ï¸ Source-specific configuration via JSON
- ğŸ§© Modular architecture for easy extension
- ğŸ ScrapingBee integration for proxy management
- ğŸ“‚ Atomic file operations for data integrity
- âš™ï¸ Robust error handling and logging
- ğŸ”— URL tracking for deduplication
- ğŸŒ Environment-based configuration
- âœ‰ï¸ Custom headers support in the config

## Directory Structure
```
ci.hls/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ selectors/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ sources/
â”‚       â”‚   â””â”€â”€ pr_news.json
â”‚       â””â”€â”€ selector_manager.py
â”œâ”€â”€ spiders/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dynamic_spider.py
â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ scrapingbee.py
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ field_extractor.py
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ json_handler.py
â”‚   â””â”€â”€ directory_manager.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ url_utils.py
â”œâ”€â”€ Data/              # Created during runtime
â”œâ”€â”€ Tracker/           # Created during runtime
â”œâ”€â”€ logs/              # Log files
â”œâ”€â”€ .env               # Environment variables
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ run.py
```

## Prerequisites
- Python 3.12+
- pip for dependency management

## Installation

1. Clone the repository:
```bash
git clone https://github.com/rishic5i/ci.hls.git
cd ci.hls
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Configure environment:
Create `.env` file with the following variables:
```env
SCRAPINGBEE_API_KEYS=key1,key2,key3
ROTATION_THRESHOLD=150
LOG_LEVEL=INFO
LOG_DIR=logs
CONCURRENT_REQUESTS=2
DOWNLOAD_DELAY=2
```

## Usage

### List All Available Sources
```bash
python run.py --list-sources
```

### Run All Sources
```bash
python run.py
```

### Run Specific Source
```bash
python run.py --source pr_news
```

### Run with ScrapingBee
```bash
# Run all sources with ScrapingBee
python run.py --use-scrapingbee

# Run specific source with ScrapingBee
python run.py --source pr_news --use-scrapingbee
```

### Environment Options
```bash
# Development mode (default)
python run.py --env dev --source pr_news

# Production mode
python run.py --env prod --source pr_news

# Combine with ScrapingBee
python run.py --env prod --source pr_news --use-scrapingbee
```

## Adding New Sources

Add your source configuration in `config/selectors/sources/` (e.g., `your_source.json`):
```json
{
    "Source Name": {
        "type": "catalog",
        "url": [
            "https://example.com/news-url"
        ],
        "selectors": {
            "source": "SOURCE_NAME",
            "news": "xpath_for_news_list",
            "abstract": "xpath_for_abstract",
            "published_date": "xpath_for_date",
            "news_link": "xpath_for_link"
        },
        "news_selector": {
            "title": "xpath_for_title",
            "article_date": "xpath_for_article_date",
            "author": "xpath_for_author",
            "content": "xpath_for_content"
        }
    }
}
```

## Output Structure

### Data Storage
```
Data/
â””â”€â”€ DD-MM-YYYY/
    â””â”€â”€ source_name.json
```

### Output Format
```json
{
    "source": "source_name",
    "category": "category",
    "published_date": "formatted_date",
    "article_date": "formatted_date",
    "title": "article_title",
    "author": "author_name",
    "abstract": "article_abstract",
    "detailed_news": "article_content",
    "article_url": "url",
    "attachments": "attachment_url",
    "contacts": "contact_details",
    "scraping_date": "YYYY-MM-DD",
    "scraping_time": "HH:MM"
}
```

## URL Tracking
- URLs are tracked in `Tracker/source_name_tracker.json`
- Prevents duplicate scraping of articles

## Error Handling
- Comprehensive error logging in the logs directory
- Automatic retry for failed requests
- Proxy rotation with ScrapingBee integration

## Contributing
1. Fork the repository
2. Create your feature branch
3. Submit pull request

## License
MIT
